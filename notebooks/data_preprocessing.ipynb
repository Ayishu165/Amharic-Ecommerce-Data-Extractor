{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7654fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import demoji\n",
    "import html\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878f93fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"D:\\kaimtenx\\project\\week4\\Amharic-Ecommerce-Data-Extractor\\data\\cleaned_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be81606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel</th>\n",
       "      <th>date</th>\n",
       "      <th>message</th>\n",
       "      <th>views</th>\n",
       "      <th>sender_id</th>\n",
       "      <th>media_type</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@Fashiontera</td>\n",
       "      <td>2025-06-14 13:32:36+00:00</td>\n",
       "      <td>„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nMade in Vietnam \\nüî∏üî∏üî∏üî∏üî∏üî∏\\nSize...</td>\n",
       "      <td>1192</td>\n",
       "      <td>-1001175527648</td>\n",
       "      <td>photo</td>\n",
       "      <td>['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'Made', 'in', 'Vietnam', 'üî∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Fashiontera</td>\n",
       "      <td>2025-06-13 19:55:32+00:00</td>\n",
       "      <td>„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nAirforce \\nMade in Vietnam \\nüî∏...</td>\n",
       "      <td>1098</td>\n",
       "      <td>-1001175527648</td>\n",
       "      <td>photo</td>\n",
       "      <td>['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'Airforce', 'Made', 'in', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@Fashiontera</td>\n",
       "      <td>2025-06-11 20:11:16+00:00</td>\n",
       "      <td>„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nUnder Armur\\nMade in Vietnam \\...</td>\n",
       "      <td>1117</td>\n",
       "      <td>-1001175527648</td>\n",
       "      <td>photo</td>\n",
       "      <td>['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'Under', 'Armur', 'Made', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Fashiontera</td>\n",
       "      <td>2025-06-04 19:18:01+00:00</td>\n",
       "      <td>„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nNB\\nMade in Vietnam \\nüî∏üî∏üî∏üî∏üî∏üî∏\\n...</td>\n",
       "      <td>1835</td>\n",
       "      <td>-1001175527648</td>\n",
       "      <td>photo</td>\n",
       "      <td>['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'NB', 'Made', 'in', 'Vietna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Fashiontera</td>\n",
       "      <td>2025-06-03 19:34:21+00:00</td>\n",
       "      <td>„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nHigh Quality shoes \\nüî∏üî∏üî∏üî∏üî∏üî∏\\nS...</td>\n",
       "      <td>1622</td>\n",
       "      <td>-1001175527648</td>\n",
       "      <td>photo</td>\n",
       "      <td>['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'High', 'Quality', 'shoes',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        channel                       date  \\\n",
       "0  @Fashiontera  2025-06-14 13:32:36+00:00   \n",
       "1  @Fashiontera  2025-06-13 19:55:32+00:00   \n",
       "2  @Fashiontera  2025-06-11 20:11:16+00:00   \n",
       "3  @Fashiontera  2025-06-04 19:18:01+00:00   \n",
       "4  @Fashiontera  2025-06-03 19:34:21+00:00   \n",
       "\n",
       "                                             message  views      sender_id  \\\n",
       "0  „Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nMade in Vietnam \\nüî∏üî∏üî∏üî∏üî∏üî∏\\nSize...   1192 -1001175527648   \n",
       "1  „Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nAirforce \\nMade in Vietnam \\nüî∏...   1098 -1001175527648   \n",
       "2  „Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nUnder Armur\\nMade in Vietnam \\...   1117 -1001175527648   \n",
       "3  „Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nNB\\nMade in Vietnam \\nüî∏üî∏üî∏üî∏üî∏üî∏\\n...   1835 -1001175527648   \n",
       "4  „Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è\\nHigh Quality shoes \\nüî∏üî∏üî∏üî∏üî∏üî∏\\nS...   1622 -1001175527648   \n",
       "\n",
       "  media_type                                             tokens  \n",
       "0      photo  ['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'Made', 'in', 'Vietnam', 'üî∏...  \n",
       "1      photo  ['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'Airforce', 'Made', 'in', '...  \n",
       "2      photo  ['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'Under', 'Armur', 'Made', '...  \n",
       "3      photo  ['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'NB', 'Made', 'in', 'Vietna...  \n",
       "4      photo  ['„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è„Ä∞Ô∏è', 'High', 'Quality', 'shoes',...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"D:\\kaimtenx\\project\\week4\\Amharic-Ecommerce-Data-Extractor\\data\\cleaned_data.csv\")\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc42120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14453 entries, 0 to 14452\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   channel     14453 non-null  object\n",
      " 1   date        14453 non-null  object\n",
      " 2   message     14453 non-null  object\n",
      " 3   views       14453 non-null  int64 \n",
      " 4   sender_id   14453 non-null  int64 \n",
      " 5   media_type  14291 non-null  object\n",
      " 6   tokens      14453 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 790.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#display\n",
    "(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f20dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716f6aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayish\\AppData\\Local\\Temp\\ipykernel_2180\\1768192006.py:9: MarkupResemblesLocatorWarning: The input passed in on this line looks more like a URL than HTML or XML.\n",
      "\n",
      "If you meant to use Beautiful Soup to parse the web page found at a certain URL, then something has gone wrong. You should use an Python package like 'requests' to fetch the content behind the URL. Once you have the content as a string, you can feed that string into Beautiful Soup.\n",
      "\n",
      "However, if you want to parse some data that happens to look like a URL, then nothing has gone wrong: you are using Beautiful Soup correctly, and this warning is spurious and can be filtered. To make this warning go away, run this code before calling the BeautifulSoup constructor:\n",
      "\n",
      "    from bs4 import MarkupResemblesLocatorWarning\n",
      "    import warnings\n",
      "\n",
      "    warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning)\n",
      "    \n",
      "  text = BeautifulSoup(text, 'html.parser').get_text()\n"
     ]
    }
   ],
   "source": [
    "#text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"Complete text cleaning pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "        # Convert HTML entities\n",
    "    text = html.unescape(text)\n",
    "        # Remove HTML tags\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "        # Remove emojis\n",
    "    text = demoji.replace(text, '')\n",
    "        # Remove URLs and Telegram mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|t\\.me/\\S+|@\\w+', '', text)\n",
    "        # Remove decorative characters (like „Ä∞Ô∏è)\n",
    "    text = re.sub(r'[„Ä∞Ô∏èüî∏]+', ' ', text)\n",
    "        # Remove extra newlines and normalize whitespace\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    # Normalize Amharic orthographic variants\n",
    "    norm_map = {\n",
    "        '[·àê·äÄ]': '·àÄ',\n",
    "        '[·ä†·ä£]': '·ä†',\n",
    "        '[·å∏·çÄ]': '·å∏',\n",
    "    }\n",
    "    for pattern, repl in norm_map.items():\n",
    "        text = re.sub(pattern, repl, text)\n",
    "\n",
    "    # Remove foreign chars except Ethiopic letters/digits/punctuations\n",
    "    text = re.sub(r\"[^\\u1200-\\u137F0-9·ç°·ç¢·ç£·ç§·ç•·ç¶·çß·ç® \\n]\", '', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.strip()\n",
    "# Apply cleaning\n",
    "df['clean_message'] = df['message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1646ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokinization\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text while handling special cases\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Handle common product patterns\n",
    "    text = re.sub(r'(\\bMade in \\w+)', r'\\1', text)  # Protect \"Made in Vietnam\"\n",
    "    text = re.sub(r'(\\bSize [A-Z0-9]+\\b)', r'\\1', text)  # Protect \"Size XL\"\n",
    "    \n",
    "    # Basic tokenization\n",
    "    tokens = re.findall(r'\\b[\\w-]+\\b', text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization\n",
    "df['tokens'] = df['clean_message'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b09a948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       clean_message  \\\n",
      "0  4142 3400 ·àµ·àç·ä≠ 251945355266 ·çã·àΩ·äï ·â∞·à´ ·ä†·ãµ·à´·àª ·ä†·ã≤·àµ ·ä†·â†·â£...   \n",
      "1  4041424344 4500 ·àµ·àç·ä≠ 251945355266 ·çã·àΩ·äï ·â∞·à´ ·ä†·ãµ·à´·àª ·ä†...   \n",
      "2  4142 3500 ·àµ·àç·ä≠ 251945355266 ·çã·àΩ·äï ·â∞·à´ ·ä†·ãµ·à´·àª ·ä†·ã≤·àµ ·ä†·â†·â£...   \n",
      "3  4243 2900 ·àµ·àç·ä≠ 251945355266 ·çã·àΩ·äï ·â∞·à´ ·ä†·ãµ·à´·àª ·ä†·ã≤·àµ ·ä†·â†·â£...   \n",
      "4  44 3500 ·àµ·àç·ä≠ 251945355266 ·çã·àΩ·äï ·â∞·à´ ·ä†·ãµ·à´·àª ·ä†·ã≤·àµ ·ä†·â†·â£ ·å¶...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [4142, 3400, ·àµ·àç·ä≠, 251945355266, ·çã·àΩ·äï, ·â∞·à´, ·ä†·ãµ·à´·àª,...  \n",
      "1  [4041424344, 4500, ·àµ·àç·ä≠, 251945355266, ·çã·àΩ·äï, ·â∞·à´,...  \n",
      "2  [4142, 3500, ·àµ·àç·ä≠, 251945355266, ·çã·àΩ·äï, ·â∞·à´, ·ä†·ãµ·à´·àª,...  \n",
      "3  [4243, 2900, ·àµ·àç·ä≠, 251945355266, ·çã·àΩ·äï, ·â∞·à´, ·ä†·ãµ·à´·àª,...  \n",
      "4  [44, 3500, ·àµ·àç·ä≠, 251945355266, ·çã·àΩ·äï, ·â∞·à´, ·ä†·ãµ·à´·àª, ·ä†...  \n"
     ]
    }
   ],
   "source": [
    "print(df[['clean_message', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8ef9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entity Extraction\n",
    "def extract_entities(tokens):\n",
    "    \"\"\"Identify products, brands, and attributes\"\"\"\n",
    "    entities = {\n",
    "        'brands': [],\n",
    "        'attributes': [],\n",
    "        'materials': []\n",
    "    }\n",
    "        # Common brand names in fashion\n",
    "    fashion_brands = {'nike', 'adidas', 'under armour', 'new balance', 'puma', 'armur'}\n",
    "        # Common product attributes\n",
    "    attributes = {'size', 'made in', 'high quality', 'color', 'xl', 'xxl'}\n",
    "        # Join tokens for multi-word matching\n",
    "    text = ' '.join(tokens).lower()\n",
    "        # Extract brands\n",
    "    for brand in fashion_brands:\n",
    "        if brand in text:\n",
    "            entities['brands'].append(brand)\n",
    "    \n",
    "    # Extract attributes\n",
    "    for attr in attributes:\n",
    "        if attr in text:\n",
    "            entities['attributes'].append(attr)\n",
    "    \n",
    "    # Extract materials (simple heuristic)\n",
    "    materials = {'leather', 'cotton', 'polyester', 'vietnam'}\n",
    "    for token in tokens:\n",
    "        if token.lower() in materials:\n",
    "            entities['materials'].append(token.lower())\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Apply entity extraction\n",
    "df['entities'] = df['tokens'].apply(extract_entities)\n",
    "\n",
    "# Create expanded columns\n",
    "df['brands'] = df['entities'].apply(lambda x: '|'.join(x['brands']))\n",
    "df['attributes'] = df['entities'].apply(lambda x: '|'.join(x['attributes']))\n",
    "df['materials'] = df['entities'].apply(lambda x: '|'.join(x['materials']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ed910db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata processing\n",
    "# Extract datetime components\n",
    "df['date_only'] = df['date'].dt.date\n",
    "df['time_only'] = df['date'].dt.time\n",
    "df['day_of_week'] = df['date'].dt.day_name()\n",
    "df['hour'] = df['date'].dt.hour\n",
    "# Create time categories\n",
    "time_labels = ['Late Night (12AM-6AM)', 'Morning (6AM-12PM)', \n",
    "               'Afternoon (12PM-6PM)', 'Evening (6PM-12AM)']\n",
    "df['time_category'] = pd.cut(\n",
    "    df['hour'],\n",
    "    bins=[0, 6, 12, 18, 24],\n",
    "    labels=time_labels,\n",
    "    right=False\n",
    ")\n",
    "\n",
    "# Clean up columns\n",
    "df.drop(columns=['hour', 'entities'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "316f1a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Data saved to CSV and JSON files.\n"
     ]
    }
   ],
   "source": [
    "# Final Data Structure and Export\n",
    "# Select and order final columns\n",
    "final_columns = [\n",
    "    # Metadata\n",
    "    'channel', 'date', 'date_only', 'time_only', \n",
    "    'day_of_week', 'time_category', 'sender_id', \n",
    "    'views', 'media_type',\n",
    "    \n",
    "    # Content\n",
    "    'message', 'clean_message', 'tokens',\n",
    "    \n",
    "    # Entities\n",
    "    'brands', 'attributes', 'materials'\n",
    "]\n",
    "\n",
    "processed_df = df[final_columns]\n",
    "\n",
    "# Save to CSV\n",
    "processed_df.to_csv('../data/cleaned_processed_data.csv', index=False)\n",
    "\n",
    "# Save to JSON\n",
    "processed_df.to_json('../data/cleaned_processed_dat.json', orient='records', force_ascii=False)\n",
    "\n",
    "print(\"Processing complete. Data saved to CSV and JSON files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
