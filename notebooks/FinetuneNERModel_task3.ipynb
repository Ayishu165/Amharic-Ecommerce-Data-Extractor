{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate seqeval -q\n",
        "\n",
        "# ## 2. Import Libraries\n",
        "import pandas as pd\n",
        "from datasets import Dataset, Features, Sequence, Value, ClassLabel\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "from seqeval.metrics import classification_report\n",
        "import numpy as np # For compute_metrics\n"
      ],
      "metadata": {
        "id": "-e3jZVkPwZDC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Task 3: Fine-Tune NER Model for Amharic\n",
        "# ## 3. Define Labels and ID-to-Label Mapping\n",
        "# These labels correspond to the CoNLL format and your entity types.\n",
        "labels = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
        "id2label = {i: label for i, label in enumerate(labels)}\n",
        "label2id = {label: i for i, label in enumerate(labels)}\n",
        "\n",
        "# ## 4. Load the Labeled Dataset in CoNLL Format\n",
        "# We'll use the simulated CoNLL data generated in the previous step.\n",
        "# In a real scenario, you would load this from a file, e.g., `labeled_data.txt`.\n",
        "\n",
        "# --- START OF SIMULATED CoNLL DATA ---\n",
        "# This data is a placeholder. Replace it with the content of your\n",
        "# `labeled_data.txt` from Task 2 for real training.\n",
        "conll_data_string = \"\"\"\n",
        "አዲስ\tB-LOC\n",
        "አበባ\tI-LOC\n",
        "ላይ\tO\n",
        "ጥሩ\tO\n",
        "ጥራት\tO\n",
        "ያለው\tO\n",
        "የህፃናት\tB-Product\n",
        "አልጋ\tI-Product\n",
        "በ2500\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ብቻ።\tO\n",
        "\n",
        "የሴቶች\tB-Product\n",
        "ቦርሳ\tI-Product\n",
        "በ1200\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ቦሌ\tB-LOC\n",
        "አካባቢ\tO\n",
        "ይገኛል።\tO\n",
        "\n",
        "ሳሪስ\tB-LOC\n",
        "ለገበያ\tO\n",
        "የቀረበ\tO\n",
        "ዘመናዊ\tB-Product\n",
        "ቲቪ\tI-Product\n",
        "8000\tB-PRICE\n",
        "ብር።\tI-PRICE\n",
        "\n",
        "ለቤትዎ\tO\n",
        "ውበት\tO\n",
        "የሆኑ\tO\n",
        "መጋረጃዎች\tB-Product\n",
        "በ650\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ከፒያሳ።\tB-LOC\n",
        "\n",
        "የወንዶች\tB-Product\n",
        "ሸሚዝ\tI-Product\n",
        "በ950\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "አዲስ\tB-LOC\n",
        "አበባ።\tI-LOC\n",
        "\n",
        "ጥሩ\tO\n",
        "ስልክ\tB-Product\n",
        "በ1000\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ቦሌ\tB-LOC\n",
        "ላይ።\tO\n",
        "\n",
        "ላፕቶፕ\tB-Product\n",
        "በ15000\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ሳሪስ\tB-LOC\n",
        "አካባቢ።\tO\n",
        "\n",
        "ለልጆች\tO\n",
        "መጫወቻዎች\tB-Product\n",
        "በ300\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ከፒያሳ።\tB-LOC\n",
        "\n",
        "የተለያዩ\tO\n",
        "የቤት\tB-Product\n",
        "እቃዎች\tI-Product\n",
        "7000\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "አዲስ\tB-LOC\n",
        "አበባ።\tI-LOC\n",
        "\n",
        "ፋሽን\tB-Product\n",
        "ልብሶች\tI-Product\n",
        "800\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ቦሌ።\tB-LOC\n",
        "\n",
        "የህፃናት\tB-Product\n",
        "መጫወቻዎች\tI-Product\n",
        "በ450\tB-PRICE\n",
        "ብር\tI-PRICE\n",
        "ሳሪስ።\tB-LOC\n",
        "\"\"\"\n",
        "# --- END OF SIMULATED CoNLL DATA ---\n",
        "\n",
        "def parse_conll(conll_string):\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "    for line in conll_string.strip().split('\\n'):\n",
        "        if line.strip() == \"\":\n",
        "            if current_sentence:\n",
        "                sentences.append(current_sentence)\n",
        "            current_sentence = []\n",
        "        else:\n",
        "            parts = line.split('\\t')\n",
        "            if len(parts) == 2: # Ensure it's a token-label pair\n",
        "                current_sentence.append(parts)\n",
        "    if current_sentence: # Add the last sentence if not followed by a blank line\n",
        "        sentences.append(current_sentence)\n",
        "    return sentences\n",
        "\n",
        "parsed_data = parse_conll(conll_data_string)\n",
        "\n",
        "# Convert parsed data to a format suitable for Hugging Face `datasets`\n",
        "processed_data = []\n",
        "for sentence_data in parsed_data:\n",
        "    tokens = [item[0] for item in sentence_data]\n",
        "    # Convert labels to numerical IDs. Handle potential KeyError if label not found.\n",
        "    ner_tags = []\n",
        "    for item in sentence_data:\n",
        "        label_str = item[1]\n",
        "        if label_str in label2id:\n",
        "            ner_tags.append(label2id[label_str])\n",
        "        else:\n",
        "            # Handle unknown labels if any, perhaps by assigning to O or logging\n",
        "            ner_tags.append(label2id[\"O\"]) # Default to 'O' for safety\n",
        "            print(f\"Warning: Unknown label '{label_str}' encountered. Assigned 'O'.\")\n",
        "\n",
        "    processed_data.append({\"tokens\": tokens, \"ner_tags\": ner_tags})\n",
        "\n",
        "# Create a Hugging Face Dataset\n",
        "features = Features({\n",
        "    'tokens': Sequence(Value('string')),\n",
        "    'ner_tags': Sequence(ClassLabel(names=labels))\n",
        "})\n",
        "\n",
        "dataset = Dataset.from_list(processed_data, features=features)\n",
        "\n",
        "# ## 5. Tokenize the Data and Align Labels\n",
        "# We use `XLM-Roberta-base` as the pre-trained model.\n",
        "model_checkpoint = \"xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"],\n",
        "        truncation=True,\n",
        "        is_split_into_words=True,\n",
        "        padding=\"max_length\", # Pad to max length of model's input\n",
        "        max_length=128 # Define a reasonable max length for tokens\n",
        "    )\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word_idx that is None. We set their label to -100\n",
        "            # so they are ignored in the loss function.\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to -100.\n",
        "            else:\n",
        "                label_ids.append(-100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "# ## 6. Split Dataset into Training and Validation Sets\n",
        "# For demonstration, we'll split the small simulated dataset.\n",
        "# In a real scenario, you'd have a much larger dataset for training.\n",
        "# Ensure there's enough data for both train and test. If the dataset is tiny,\n",
        "# train_test_split might result in empty sets, so add a check.\n",
        "if len(tokenized_dataset) > 1:\n",
        "    train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = train_test_split[\"train\"]\n",
        "    eval_dataset = train_test_split[\"test\"]\n",
        "else:\n",
        "    train_dataset = tokenized_dataset\n",
        "    eval_dataset = tokenized_dataset # Use the same for eval if only one sample, for demonstration\n",
        "    print(\"Warning: Dataset is too small for meaningful split. Using entire dataset for both train and eval.\")\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Evaluation examples: {len(eval_dataset)}\")\n",
        "\n",
        "\n",
        "# ## 7. Set Up Training Arguments\n",
        "# Define the output directory, learning rate, batch size, etc.\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-ner\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False, # Set to True if you want to push to Hugging Face Hub\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        ")\n",
        "\n",
        "# ## 8. Initialize Model and Trainer\n",
        "# Load the pre-trained model with the correct number of labels.\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    num_labels=len(labels),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Define a data collator to batch your inputs.\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Define compute_metrics function for evaluation\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels_ids = p # Renamed 'labels' to 'labels_ids' to avoid confusion with the global 'labels' list\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    true_labels_filtered = []\n",
        "    true_predictions_filtered = []\n",
        "\n",
        "    for sentence_labels, sentence_predictions in zip(labels_ids, predictions):\n",
        "        temp_true_labels = []\n",
        "        temp_true_predictions = []\n",
        "        for label_id, pred_id in zip(sentence_labels, sentence_predictions):\n",
        "            if label_id != -100: # Only consider tokens that are not special tokens\n",
        "                temp_true_labels.append(label_id)\n",
        "                temp_true_predictions.append(pred_id)\n",
        "        true_labels_filtered.append(temp_true_labels)\n",
        "        true_predictions_filtered.append(temp_true_predictions)\n",
        "\n",
        "    # Convert numeric IDs back to original labels for seqeval\n",
        "    true_labels_decoded = [[id2label[l_id] for l_id in sublist] for sublist in true_labels_filtered]\n",
        "    true_predictions_decoded = [[id2label[p_id] for p_id in sublist] for sublist in true_predictions_filtered]\n",
        "\n",
        "    return {\n",
        "        \"f1\": f1_score(true_labels_decoded, true_predictions_decoded),\n",
        "        \"precision\": precision_score(true_labels_decoded, true_predictions_decoded),\n",
        "        \"recall\": recall_score(true_labels_decoded, true_predictions_decoded),\n",
        "    }\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ## 9. Fine-Tune the Model\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# ## 10. Evaluate the Fine-Tuned Model\n",
        "print(\"\\n*** Model Evaluation Results ***\")\n",
        "results = trainer.evaluate()\n",
        "print(results)\n",
        "\n",
        "# A more detailed classification report\n",
        "predictions, labels_ids, _ = trainer.predict(eval_dataset) # Renamed for clarity\n",
        "predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "true_labels_filtered_report = []\n",
        "true_predictions_filtered_report = []\n",
        "\n",
        "for sentence_labels, sentence_predictions in zip(labels_ids, predictions):\n",
        "    temp_true_labels = []\n",
        "    temp_true_predictions = []\n",
        "    for label_id, pred_id in zip(sentence_labels, sentence_predictions):\n",
        "        if label_id != -100:\n",
        "            temp_true_labels.append(label_id)\n",
        "            temp_true_predictions.append(pred_id)\n",
        "    true_labels_filtered_report.append(temp_true_labels)\n",
        "    true_predictions_filtered_report.append(temp_true_predictions)\n",
        "\n",
        "true_labels_decoded = [[id2label[l_id] for l_id in sublist] for sublist in true_labels_filtered_report]\n",
        "true_predictions_decoded = [[id2label[p_id] for p_id in sublist] for sublist in true_predictions_filtered_report]\n",
        "\n",
        "\n",
        "# Handle cases where true_labels_decoded or true_predictions_decoded might be empty\n",
        "if any(true_labels_decoded) and any(true_predictions_decoded):\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(true_labels_decoded, true_predictions_decoded, zero_division=0))\n",
        "else:\n",
        "    print(\"\\nNo entities found in evaluation set for classification report.\")\n",
        "\n",
        "# ## 11. Save the Model and Tokenizer\n",
        "output_dir = \"./finetuned_ner_model\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model and tokenizer saved to {output_dir}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/11 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e46fc3abce914b3b8f06a809a0134ac8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training examples: 8\n",
            "Evaluation examples: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/tmp/ipython-input-2-383071823.py:286: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmuay2015\u001b[0m (\u001b[33mdmuay2015-debre-markos-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250625_110414-1vplx1cf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/dmuay2015-debre-markos-university/huggingface/runs/1vplx1cf' target=\"_blank\">xlm-roberta-base-finetuned-ner</a></strong> to <a href='https://wandb.ai/dmuay2015-debre-markos-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/dmuay2015-debre-markos-university/huggingface' target=\"_blank\">https://wandb.ai/dmuay2015-debre-markos-university/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/dmuay2015-debre-markos-university/huggingface/runs/1vplx1cf' target=\"_blank\">https://wandb.ai/dmuay2015-debre-markos-university/huggingface/runs/1vplx1cf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 07:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.999116</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.978674</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.968409</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.095238</td>\n",
              "      <td>0.222222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training complete.\n",
            "\n",
            "*** Model Evaluation Results ***\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 1.9991164207458496, 'eval_f1': 0.13333333333333333, 'eval_precision': 0.09523809523809523, 'eval_recall': 0.2222222222222222, 'eval_runtime': 1.285, 'eval_samples_per_second': 2.335, 'eval_steps_per_second': 0.778, 'epoch': 3.0}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.00      0.00      0.00         3\n",
            "       PRICE       0.00      0.00      0.00         3\n",
            "     Product       0.10      0.67      0.17         3\n",
            "\n",
            "   micro avg       0.10      0.22      0.13         9\n",
            "   macro avg       0.03      0.22      0.06         9\n",
            "weighted avg       0.03      0.22      0.06         9\n",
            "\n",
            "Model and tokenizer saved to ./finetuned_ner_model\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864,
          "referenced_widgets": [
            "e46fc3abce914b3b8f06a809a0134ac8",
            "18c8cee370b24b80a875819712387a20",
            "ac2ab937a7ee405fa583d5b5408692c8",
            "63c2ddded43a44af99ec40a55c39b2a2",
            "e96d65628fd1425c863ba1e2f0e34fa6",
            "8605dac0603f4a6cbb36ee039eb4517d",
            "da078978d57e4beca574690ce324bec8",
            "1c766854e1c84c43ba5bffc95fbbe0f9",
            "c761058fb532408f8f78af36d398a01b",
            "b83bf7baa21b4116a9fe93b788c51d6d",
            "c3c43c4c73c440e7a653c3eec9510bc3"
          ]
        },
        "id": "vJMk9FEPtYKy",
        "outputId": "7001177d-f816-4cca-bd05-f49f9647f90b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "google_drive_save_path = \"/content/drive/MyDrive/my_ner_models/amharic_ner_xlmroberta\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(google_drive_save_path, exist_ok=True)\n",
        "print(f\"Ensured directory exists: {google_drive_save_path}\")\n",
        "\n",
        "# ## 3. Save the Model and Tokenizer\n",
        "print(f\"Saving model and tokenizer to Google Drive at: {google_drive_save_path}\")\n",
        "try:\n",
        "    model.save_pretrained(google_drive_save_path)\n",
        "    tokenizer.save_pretrained(google_drive_save_path)\n",
        "    print(\"Model and tokenizer successfully saved to Google Drive!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model to Google Drive: {e}\")\n",
        "    print(\"Please ensure your Drive is mounted correctly and the path is valid.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jtFU7PJyx7c",
        "outputId": "0fbddd96-a4f4-4aa8-e072-bf705ad96434"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Ensured directory exists: /content/drive/MyDrive/my_ner_models/amharic_ner_xlmroberta\n",
            "Saving model and tokenizer to Google Drive at: /content/drive/MyDrive/my_ner_models/amharic_ner_xlmroberta\n",
            "Model and tokenizer successfully saved to Google Drive!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e46fc3abce914b3b8f06a809a0134ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18c8cee370b24b80a875819712387a20",
              "IPY_MODEL_ac2ab937a7ee405fa583d5b5408692c8",
              "IPY_MODEL_63c2ddded43a44af99ec40a55c39b2a2"
            ],
            "layout": "IPY_MODEL_e96d65628fd1425c863ba1e2f0e34fa6"
          }
        },
        "18c8cee370b24b80a875819712387a20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8605dac0603f4a6cbb36ee039eb4517d",
            "placeholder": "​",
            "style": "IPY_MODEL_da078978d57e4beca574690ce324bec8",
            "value": "Map: 100%"
          }
        },
        "ac2ab937a7ee405fa583d5b5408692c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c766854e1c84c43ba5bffc95fbbe0f9",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c761058fb532408f8f78af36d398a01b",
            "value": 11
          }
        },
        "63c2ddded43a44af99ec40a55c39b2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b83bf7baa21b4116a9fe93b788c51d6d",
            "placeholder": "​",
            "style": "IPY_MODEL_c3c43c4c73c440e7a653c3eec9510bc3",
            "value": " 11/11 [00:00&lt;00:00, 114.29 examples/s]"
          }
        },
        "e96d65628fd1425c863ba1e2f0e34fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8605dac0603f4a6cbb36ee039eb4517d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da078978d57e4beca574690ce324bec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1c766854e1c84c43ba5bffc95fbbe0f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c761058fb532408f8f78af36d398a01b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b83bf7baa21b4116a9fe93b788c51d6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3c43c4c73c440e7a653c3eec9510bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}